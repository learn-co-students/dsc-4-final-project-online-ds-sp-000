{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Presidential Speeches using Artificial Neural Networks\n",
    "When building NLP classifiers, the goal is usually to identify the class of new, unseen data. This project aims to take this one step further by not only identifying the correct speaker of presidential quotes, but to demonstrate how such an approach can provide to not just the differences, but similarities between presidents.\n",
    "\n",
    "All data for this project can be found in the folder labeled *Corpus of Presidential Speeches,* which contains over 1,000 raw text files, each of which represents a single speech given by a president. Also included are the general campaign speeches for Donald Trump and Hillary Clinton, though the data does not contain speeches given by Trump since taking office. The original corpus was can be accessed from **[The Grammar Lab](http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus)**.\n",
    "\n",
    "The term *speech* is broadly defined by The Grammar Lab, and may include other forms of public speaking, including debate transcripts. For purposes of this analysis, context of public speech is of little concern, so the corpora was accepted as is.\n",
    "\n",
    "**IMPORTANT: In order for this notebook to run properly, you will first need to download the pre-trained GloVe vectors [here](http://nlp.stanford.edu/data/glove.6B.zip). The file glove.6B.100d.txt will need to be then saved to the same folder as this notebook file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, string, re\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select Presidents for Comparison\n",
    "Select any two presidents you want, as well as the minimum word count for each sample. All examples previously used 50 word samples, but you can choose any size you like. Larger samples may increase accuracy, but will also make it more difficult to understand how and why the network got it wrong. It will also result in fewer samples, meaning there may be greater levels of variance in the results. Maybe try a few different options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presidents():\n",
    "    # Create dictionary with each key representing an index number, and each value representing a president name\n",
    "    # Names are taken from directory of folders, each of which contains the corpus of speeches for that individual\n",
    "    \n",
    "    i=1\n",
    "    selection_index = {}\n",
    "    for pres in sorted(list(os.listdir('Corpus of Presidential Speeches/')))[1:]:\n",
    "        print(f'{i}:\\t{pres.title()}')\n",
    "        selection_index[i] = pres\n",
    "        i += 1\n",
    "    \n",
    "    # Get input from user to choose presidents to compare, along with minimum vocabulary size for each sample\n",
    "    selections = [input('\\n\\nEnter the number of the first president/candidate you would like to compare:\\t')]\n",
    "    selections.append(input('Enter the number of the second president/candidate you would like to compare:\\t'))\n",
    "    selections = [selection_index[int(selections[0])], selection_index[int(selections[1])]]\n",
    "    num_words = int(input('What is the minimum number of words that should be included in each sample?\\t'))\n",
    "    print(f'\\n\\nSelection 1:\\t\\t{selections[0].title()}\\nSelection 2:\\t\\t{selections[1].title()}\\nVocabulary Size:\\t{num_words}')\n",
    "    return sorted(selections, reverse=True), num_words\n",
    "\n",
    "selections, num_words = get_presidents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse Text and Save Samples to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer to get total time for running all code\n",
    "t0 = datetime.datetime.now()\n",
    "\n",
    "# Save selections to variables\n",
    "p1, p2 = sorted([selections[0], selections[1]])\n",
    "\n",
    "# Create variable that identifies directory containing speech corpora\n",
    "lvl1 = 'Corpus of Presidential Speeches/'\n",
    "\n",
    "# Create dictionary to store samples. Will be converted to dataframe later.\n",
    "speech_dict = {'president': [], 'name': [], 'text': []}\n",
    "\n",
    "# Create dictionaries to keep track of labels and get speech count\n",
    "labels_index = {}\n",
    "speech_count = {}\n",
    "\n",
    "# Iterate through each speech of selected presidents and save results to dictionaries created above\n",
    "i=0\n",
    "for folder in sorted(os.listdir(lvl1)):\n",
    "    if folder in selections:\n",
    "        labels_index[i] = folder.lower()\n",
    "        speech_count[folder] = [0]\n",
    "        \n",
    "        for file in os.listdir(lvl1 + folder):\n",
    "            speech_count[folder][0] += 1\n",
    "            \n",
    "            with open(f'{lvl1}{folder}/{file}', 'r') as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                # Remove tags for date, title, etc. and select punctuation\n",
    "                no_tags = re.sub('<[^>]+>', '', data)\n",
    "                no_line_breaks = no_tags.replace('\\n', ' ')\n",
    "                no_ellipses = no_line_breaks.replace('...', ' ')\n",
    "                no_dashes = no_ellipses.replace('-', ' ')\n",
    "                \n",
    "                # Split data into sentences\n",
    "                sentences = no_dashes.split('. ')\n",
    "                \n",
    "                # Create samples based on word count selection per sample\n",
    "                # Sentences added to single sample until min word count is reached\n",
    "                while len(sentences) > 0:\n",
    "                    text = []\n",
    "                    while (len(text) <= num_words) and (len(sentences) > 0):\n",
    "                        sentence = sentences.pop(0)\n",
    "                        \n",
    "                        # Add period at end if ending character is not a question mark or exclamation point\n",
    "                        if (len(sentence) > 0) and (sentence[-1] not in ['?', '!']):\n",
    "                            sentence = sentence + '.'\n",
    "                            \n",
    "                        text += sentence.split()\n",
    "                        \n",
    "                    speech_dict['text'].append(' '.join(text))\n",
    "                    speech_dict['president'].append(i)\n",
    "                    speech_dict['name'].append(folder.title())\n",
    "                    \n",
    "        i+=1\n",
    "                \n",
    "df = pd.DataFrame.from_dict(speech_dict)\n",
    "df['length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Final samples from each speech may not contain enough words. \n",
    "# Drop rows that do not contain minimum number of words\n",
    "df = df[df['length'] >= num_words]\n",
    "\n",
    "# Get number of classes in case comparing more than 2 in the future\n",
    "num_classes = len(df['president'].unique())\n",
    "\n",
    "print(df.shape)\n",
    "print(labels_index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "Text preprocessing was primarily adapted from the following blog post: [A Practitioner's Guide to Natual Language Processing](https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72).\n",
    "\n",
    "Functions will be created for each step of preprocessing:\n",
    "1. Expand Contractions\n",
    "2. Remove Special Characters\n",
    "3. Lemmatize Text\n",
    "4. Remove Stop Words\n",
    "5. Tokenize words\n",
    "\n",
    "### Expand Cotractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    # Cxpand contractions into individual words using pre-defined list of contractions.\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        # Find words in text that match with contraction map, and return expanded text\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    # Remove special characters from text\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Get base form for word variants (i.e. running ==> run, cats ==> cat, etc.)\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords & Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Remove words 'no' and 'not' from stopword list as these may change meaning of words/phrases\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "for word in ['no', 'not', 'he', 'she', 'his', 'her', 'hers']:\n",
    "    stopword_list.remove(word)\n",
    "\n",
    "# Add characters not captured in special character removal to stop list\n",
    "stopword_list += ['[', ']', \"\\\\\"]                     \n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Preprocessing Functions to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "clean_text = []\n",
    "\n",
    "for text in df['text']:\n",
    "    # Remove accent characters => Expand Contractions => Remove special characters => Lammatize Text => Remove Stop Words\n",
    "    no_accent_chars = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    expanded_text = expand_contractions(no_accent_chars)\n",
    "    no_special_chars = remove_special_characters(expanded_text, remove_digits=False)\n",
    "    lem_text = lemmatize_text(no_special_chars)\n",
    "    filtered_text = remove_stopwords(lem_text)\n",
    "    \n",
    "    # Make all text lowercase\n",
    "    lowercase_text = filtered_text.lower()\n",
    "    \n",
    "    # Split text into list of words\n",
    "    words = lowercase_text.split()\n",
    "    clean_text.append(words)\n",
    "\n",
    "# Add cleaned text to dataframe\n",
    "df['clean_text'] = clean_text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Data\n",
    "The primary concern with this data is that it may be imbalanced. If one individual gave significantly more speeches than the other, then our network may learn that it can maximize accuracy be simply guessing the more prominent speaker more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "sns.countplot('name', data=df)\n",
    "plt.title(f'Sample Count by {df.columns[0].title()}')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.xlabel(df.columns[0].title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Dataset\n",
    "Because it is clear that one individual has significantly more samples than the other, it is best to select a subset of samples equivalent to the total sample size of the less prominent individual. This could later prove to be problematic for presidents who gave a limited number of public appearances, as this could result in there not being enough samples to generate meaningful results. In this case, it is not expected to be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Dataset.\n",
    "shuffled_df = df.sample(frac=1)\n",
    "\n",
    "# Create dictionary to store scaled dataframes for each president.\n",
    "scaled_dfs = {}\n",
    "\n",
    "# Set max samples based on president with fewest samples\n",
    "n = shuffled_df.loc[shuffled_df['name'] == df['name'].value_counts().index[-1]].shape[0]\n",
    "\n",
    "#Randomly select appropriate number of observations from the majority class\n",
    "for name in df['name'].unique():\n",
    "    scaled_dfs[f'{name}_df'] = shuffled_df.loc[shuffled_df['name'] == name].sample(n=n)\n",
    "\n",
    "# Concatenate dataframes\n",
    "scaled_dfs = [scaled_dfs[key] for key in list(scaled_dfs.keys())]\n",
    "normalized_df = pd.concat(scaled_dfs)\n",
    "\n",
    "#plot the dataset after the undersampling\n",
    "plt.figure(figsize=(25, 8))\n",
    "sns.countplot('name', data=normalized_df)\n",
    "plt.title(f'Balanced Paragraph Count by {df.columns[0].title()}')\n",
    "plt.ylabel('Paragraph Count')\n",
    "plt.xlabel(df.columns[0].title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables for dependent and independent variables\n",
    "target = normalized_df.president\n",
    "data = normalized_df['clean_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bigrams\n",
    "Certain terms may come up that make more sense together than independently (i.e. \"United States\", \"North Korea\", \"Nuclear Weapon\", etc). When terms are identified next to each other frequently, they may be combined into a single term using an underscore (\"United_States\"), making it easier for our network to deal with later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import phrases\n",
    "\n",
    "# Identify and create bigrams\n",
    "bigrams = phrases.Phrases(data)\n",
    "bigrams_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    bigrams_data.append(bigrams[data[i]])\n",
    "    \n",
    "bigrams_data = np.array(bigrams_data)\n",
    "\n",
    "# Get word count for longest sample after stop words removed and bigrams created\n",
    "max_len = 0\n",
    "for bigram in bigrams_data:\n",
    "    max_len = len(bigram) if len(bigram) > max_len else max_len\n",
    "    \n",
    "print(f'Maximum Sample Word Count: {max_len}\\n')\n",
    "print(f\"Sample:\\n{' '.join([bigram for bigram in bigrams_data[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorize Samples for Analysis Using GloVe\n",
    "GloVe (short for Global Vectors Word Representation) is an open-source database of 6 billion pre-trained word vectors created by Stanford University. Rather than having to develop vector mapping for our entire vocabulary, theis database can be used to give our network a head start. Each unique word will be checked against the database to get only the vectors required for this specific dataset. Words and bigrams not found in the GloVe database will be assigned a zero vector and updated as part of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to hold set of all unique words in sample corpus\n",
    "total_vocabulary = set(word for text in bigrams_data for word in text)\n",
    "print(f'Total Words in Vocabulary: {len(total_vocabulary)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of vectors from GloVe based on total vocabulary\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in total_vocabulary:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'Total Words in Embedding Index: {len(embeddings_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    # Assign GloVe vector to each word in vocabulary, or zero vector to words/bigrams not found\n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(embeddings_index))])\n",
    "    \n",
    "    # Following required to allow for implementation of a fit method\n",
    "    # Cannot be used in SKLearn Pipeline otherwise \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmarking\n",
    "Traditional machine learning methods are much less resource intensive than neural networks, so it should be expected that a neural network will outperform them. Random forest, support vector classification, and logistic regression will be used as potential benchmarks, with the top performing model being used as the benchmark against which neural network performance will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf  = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=-1))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr  = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(embeddings_index)),\n",
    "                ('Logistic Regression', LogisticRegression(n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 cross-validations to gauge performance\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model in models]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating Neural Networks\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing import text\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenization & Sequencing\n",
    "For use in Keras, bigram data must be re-tokenized as sequences numerical indices rather than words. By rejoining each sample's list of bigrams into a string, the Keras tokenizer can handle this quickly and easily. The total words in the vocabulary should match the previous count for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sample's list of bigrams into a string\n",
    "bigrams_joined = []\n",
    "\n",
    "for bigrams in bigrams_data:\n",
    "    bigrams_joined.append(' '.join(bigrams))\n",
    "\n",
    "# Remove underscore from filters and no need to convert to lowercase\n",
    "tokenizer = text.Tokenizer(filters='!\"#%&()*+,./:;<=>?@[\\\\]^`{|}~\\t\\n', lower=False)\n",
    "tokenizer.fit_on_texts(bigrams_joined)\n",
    "sequences = tokenizer.texts_to_sequences(bigrams_joined)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Total Words in Vocabulary: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "The neural network excpects each sample to be the same shape, however each sample is not the exact same length. A sequence of zeroes will be added to the end of each sample such that the length matches that of the longest sample in the corpus. \n",
    "\n",
    "Once padding and sequencing is completed, the original text and bigram data will be added as additional columns so that they can be retrieved by their respective indices later, allower for a more throrough analysis of the results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "X_t_df = pd.DataFrame(X_t)\n",
    "X_t_df['bigrams'] = bigrams_data\n",
    "X_t_df['text'] = normalized_df.reset_index(drop=True)['text']\n",
    "X_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target variable\n",
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Because the benchmark was calculated using 2-fold cross-validation (training and testing on half the data each time), the neural networks will be trained and tested in as similar a way as possible so as to maintain integrity of the results. Testing will be performed on half the data, while training will take place on the other half (validation split can be treated as a tunable hyperparameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t_df, y, test_size=.5)\n",
    "\n",
    "X_train_text = X_train[['text', 'bigrams']]\n",
    "X_train = X_train[X_train.columns[:-2]]\n",
    "\n",
    "X_test_text = X_test[['text', 'bigrams']]\n",
    "X_test = X_test[X_test.columns[:-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization & Embedding\n",
    "While the embedding index created earlier should remain valid (mapping each word to a vector using GloVe), it is performed again here with the Keras generated tokens just as a precaution. However, while creating the embedding matrix, there will be one difference in that words not found in the GloVe index will be assigned random vectors rather than empty vectors since they represent weights that will be tuned as part of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding index using GloVe\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of random vectors which matches size of the word index\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    # For each word found in the embedding index, assign the known vector\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    # Otherwise, keep the randomly assigned vector\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1, \n",
    "                            100, weights = [embedding_matrix], \n",
    "                            input_length = max_len, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the embedding matrix created, the search for a top performing neural network can begin.\n",
    "\n",
    "### Network Development Strategy Synopsis\n",
    "Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have been shown to perform quite well for NLP classification tasks, and will therefore be used here in an attempt to identify the speakers of each sample. The two network architectures utilized here have been shown to perform well as a starting point for many NLP tasks, and allow for a significant amount of flexibility for adaptation. In addition to general architecture, the following strategies were implemented in building the network:\n",
    "\n",
    "- **Grid Search:** Each network will be initialized with a function that utilizes the embedding layer already created, and will return the model, history, results, and processing time. All relevant hyperparameters (including number of epochs, validation split, etc.) are assigned default settings, and may be adjusted as desired by the user. A grid search will iterate through different combinations of hyperparameters searching for the minimum loss.\n",
    "\n",
    "- **Loss vs Accuracy:** Through each epoch, the network will attempt to minimize the loss function. However, the best model will be selected based on its overall accuracy. This approach was selected in an effort to maintain alignment with Goodhart's Law, since accuracy is ultimately driven by the loss function.\n",
    "\n",
    "- **Epochs & Early Stopping:** Each time the model is trained, it will be set to go through 100 epochs, but will stop if validation loss does not improve within 5 sequential epochs.\n",
    "\n",
    "- **Checkpoints:** Because the total number of models generated can grow exponentially, it would be unrealistic to save each one to *.hdf5* for each set of hyperparameters. Instead, network accuracy and hyperparameters will be tracked, and the top performing model will be retrained so that predictions can be articulated. This is expected to result in small levels of variation within the results, but will nevertheless be reviewed at a later point to ensure the variations do not significantly impact the results. \n",
    "\n",
    "\n",
    "### Convolutional Network Architecture\n",
    "The following function takes in a set of hyperparameters, and uses the existing embedding layer to create, train, and test a single convolutional neural network. It returns the model, model history, results, and total processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(cdim=128, ksize=5, pool1=5, pool2=35, density=128, lr=.001, epochs=100, batch_size=128, validation_split=.3, patience=5, verbose=0):\n",
    "    \n",
    "    # Start Timer\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    # Display Hyperparameter Settings\n",
    "    print('Convolution Dimensions\\tWindow Size\\tPool 1\\t\\tPool 2\\t\\tDensity')\n",
    "    print(f'{cdim}\\t\\t\\t{ksize}\\t\\t{pool1}\\t\\t{pool2}\\t\\t{density}')\n",
    "    \n",
    "    \n",
    "    # Build Model\n",
    "    input_ = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(input_)\n",
    "    \n",
    "    x = Conv1D(cdim, ksize, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(pool1, padding='same')(x)\n",
    "    x = Conv1D(cdim, ksize, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool1, padding='same')(x)\n",
    "    x = Conv1D(cdim, ksize, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool2, padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(density, activation='relu')(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    \n",
    "    \n",
    "    # Compile Model\n",
    "    history = model.compile(loss='categorical_crossentropy', \n",
    "                            optimizer=Adam(lr=lr), \n",
    "                            metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Create Checkpoints & Stopping Parameters\n",
    "    checkpoints_path = f'cnn_best_{p1}_{p2}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(checkpoints_path, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=verbose, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='min')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   mode='min', \n",
    "                                   patience=patience)\n",
    "    \n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    \n",
    "    # Fit Model\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_split=validation_split, \n",
    "                        callbacks=callbacks, \n",
    "                        verbose=verbose)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # End Timer\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # Display Results\n",
    "    print(f'Time to Complete:\\t{end - start}')\n",
    "    print(f'Loss:\\t{results[0]:.2f}\\tAccuracy:\\t{results[1]:.4f}\\n')\n",
    "    \n",
    "    return model, history, results, end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Grid Search\n",
    "**Defining the Grid:** Due to limited resources, the grid was set up to create a wide range of hyperparameters, while simultaneously attempting to limit the total number of combinations.\n",
    "One item worth noting is that, depending on the minimum word count selected, some pooling and window sizes will not work properly. These instances will simply be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display total time elapsed since program started\n",
    "t1 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t1 - t0}\\n')\n",
    "\n",
    "# Hyperparameter grid\n",
    "cdims = [128, 256]\n",
    "ksizes = [3, 5, 7]\n",
    "pools1 = [3, 5, 7]\n",
    "pools2 = [25, 35]\n",
    "densities = [64, 128]\n",
    "\n",
    "# Create dictionary for tracking CNN results and hyperparameters\n",
    "cnn_results = {'Convolution Dimensions': [], \n",
    "               'Window Size': [], \n",
    "               'Pool 1': [], \n",
    "               'Pool 2': [], \n",
    "               'Density': [], \n",
    "               'Time': [], \n",
    "               'Loss': [], \n",
    "               'Accuracy': []}\n",
    "\n",
    "for cdim in cdims:\n",
    "    for ksize in ksizes:\n",
    "        for pool1 in pools1:\n",
    "            for pool2 in pools2:\n",
    "                for density in densities:\n",
    "                    \n",
    "                    try:\n",
    "                        cnn_model, cnn_history, cnn_eval, cnn_time = create_cnn_model(cdim=cdim, \n",
    "                                                                                      ksize=ksize, \n",
    "                                                                                      pool1=pool1, \n",
    "                                                                                      pool2=pool2, \n",
    "                                                                                      density=density)\n",
    "                        cnn_results['Convolution Dimensions'].append(cdim)\n",
    "                        cnn_results['Window Size'].append(ksize)\n",
    "                        cnn_results['Pool 1'].append(pool1)\n",
    "                        cnn_results['Pool 2'].append(pool2)\n",
    "                        cnn_results['Density'].append(density)\n",
    "                        cnn_results['Loss'].append(cnn_eval[0])\n",
    "                        cnn_results['Accuracy'].append(cnn_eval[1])\n",
    "                        cnn_results['Time'].append(cnn_time)\n",
    "                        \n",
    "                    except:\n",
    "                        # If invalid hyperparameters, pass\n",
    "                        pass\n",
    "                    \n",
    "t2 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t2 - t0}\\n')\n",
    "print(f'Time to Find Best Model: {t2 - t1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Best CNN Model\n",
    "Identify the model with the highest accuracy score, and use the associated hyperparameters to retrain the model, displaying a greater level of detail in verbosity when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from results dictionary\n",
    "cnn_results_df = pd.DataFrame.from_dict(cnn_results)\n",
    "\n",
    "# Get row values associated with highest accuracy\n",
    "best_cnn_model = cnn_results_df[cnn_results_df['Accuracy'] == cnn_results_df['Accuracy'].max()]\n",
    "\n",
    "# Save hyperparameters to variables\n",
    "cdim = best_cnn_model['Convolution Dimensions'].values[0]\n",
    "ksize = int(best_cnn_model['Window Size'].values[0])\n",
    "pool1 = int(best_cnn_model['Pool 1'].values[0])\n",
    "pool2 = int(best_cnn_model['Pool 2'].values[0])\n",
    "density = int(best_cnn_model['Density'].values[0])\n",
    "\n",
    "# Rebuild & retrain model with \n",
    "cnn_model, cnn_history, cnn_eval, cnn_time = create_cnn_model(cdim=cdim, \n",
    "                                                              ksize=ksize, \n",
    "                                                              pool1=pool1, \n",
    "                                                              pool2=pool2, \n",
    "                                                              density=density,\n",
    "                                                              lr=.0001, \n",
    "                                                              epochs=100, \n",
    "                                                              patience=5, \n",
    "                                                              verbose=1)\n",
    "\n",
    "t3 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t3 - t0}\\n')\n",
    "print(f'Time to Train Best Model: {t3 - t2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Network Architecture\n",
    "The following function takes in a set of hyperparameters, and uses the existing embedding layer to create, train, and test a single convolutional neural network. It returns the model, model history, results, and total processing time.\n",
    "\n",
    "**LSTM/GRU**: Unlike all other hyperparamters, the neuron type has an affect on the actual architecture of the network. The appropriate layer will be selected based on function input. While the default is set to *gru*, it should be noted that any other input will result in the model defaulting to *LSTM* and will not return an error.\n",
    "\n",
    "**Bidirectional RNN**: Though more resource intensive, it was decided to use Bidirectional layers to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(rnn_type='gru', units=50, drop=.5, density=50, lr=.001, epochs=100, batch_size=128, validation_split=.3, patience=5, verbose=0):\n",
    "    \n",
    "    # Start Timer\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    # Display Hyperparameter Settings\n",
    "    model_type = 'GRU' if rnn_type == 'gru' else 'LSTM'\n",
    "    print(f'Model Type:\\t{model_type}\\tUnits:\\t{units}\\tDropout Rate:\\t{drop}\\t\\tDensity: {density}')\n",
    "    \n",
    "    \n",
    "    # Build Model\n",
    "    input_ = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(input_)\n",
    "    \n",
    "    if rnn_type == 'gru':\n",
    "        x = Bidirectional(GRU(units, return_sequences=True))(embedded_sequences)\n",
    "    else:\n",
    "        x = Bidirectional(LSTM(units, return_sequences=True))(embedded_sequences)\n",
    "        \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    x = Dense(density, activation='relu')(x)\n",
    "    x = Dropout(drop)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    \n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=lr), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Create Checkpoints & Stopping Parameters\n",
    "    checkpoints_path = f'rnn_best_{p1}_{p2}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(checkpoints_path, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=verbose, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='min')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   mode='min', \n",
    "                                   patience=patience)\n",
    "    \n",
    "    callbacks = [checkpoint, early_stopping]\n",
    "    \n",
    "    \n",
    "    # Fit Model\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_split=validation_split, \n",
    "                        callbacks=callbacks, \n",
    "                        verbose=verbose)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # End Timer\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # Display Results\n",
    "    print(f'Time to Complete:\\t{end - start}')\n",
    "    print(f'Loss:\\t{results[0]:.2f}\\tAccuracy:\\t{results[1]:.4f}\\n')\n",
    "    \n",
    "    return model, history, results, end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Grid Search\n",
    "To account for the additional time required to train a Bidirectional RNN, fewer combinations of hyperparameters were used than were for the Convolutional Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = ['lstm', 'gru']\n",
    "units = [25, 50]\n",
    "drops = [.25, .5]\n",
    "densities = [100, 150]\n",
    "\n",
    "rnn_results = {'Model Type': [], 'Units': [], 'Dropout Rate': [], 'Density': [], 'Time': [], 'Loss': [], 'Accuracy': []}\n",
    "\n",
    "for rnn_type in rnn_types:\n",
    "    for unit in units:\n",
    "        for drop in drops:\n",
    "            for density in densities:\n",
    "                rnn_model, rnn_history, rnn_eval, rnn_time = create_rnn_model(rnn_type=rnn_type, \n",
    "                                                                              units=unit, \n",
    "                                                                              drop=drop, \n",
    "                                                                              density=density)\n",
    "                rnn_results['Model Type'].append(rnn_type)\n",
    "                rnn_results['Units'].append(unit)\n",
    "                rnn_results['Dropout Rate'].append(drop)\n",
    "                rnn_results['Density'].append(density)\n",
    "                rnn_results['Loss'].append(rnn_eval[0])\n",
    "                rnn_results['Accuracy'].append(rnn_eval[1])\n",
    "                rnn_results['Time'].append(rnn_time)\n",
    "                \n",
    "t4 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t4 - t0}\\n')\n",
    "print(f'Time to Find Best Model: {t4 - t3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Best RNN Model\n",
    "Identify the model with the highest accuracy score, and use the associated hyperparameters to retrain the model, displaying a greater level of detail in verbosity when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_results_df = pd.DataFrame.from_dict(rnn_results)\n",
    "\n",
    "best_rnn_model = rnn_results_df[rnn_results_df['Loss'] == rnn_results_df['Loss'].min()]\n",
    "\n",
    "rnn_type = best_rnn_model['Model Type'].values[0]\n",
    "units = int(best_rnn_model['Units'].values[0])\n",
    "drop = float(best_rnn_model['Dropout Rate'].values[0])\n",
    "density = int(best_rnn_model['Density'].values[0])\n",
    "\n",
    "rnn_model, rnn_history, rnn_eval, rnn_time = create_rnn_model(rnn_type=rnn_type, \n",
    "                                                              units=units, \n",
    "                                                              drop=drop, \n",
    "                                                              density=density, \n",
    "                                                              lr=.001, \n",
    "                                                              epochs=100, \n",
    "                                                              patience=5, \n",
    "                                                              verbose=1)\n",
    "\n",
    "t5 = datetime.datetime.now()\n",
    "print(f'Time Elapsed: {t5 - t0}\\n')\n",
    "print(f'Time to Train Best Model: {t5 - t4}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "labels = [f'Benchmark: {max(scores,key=itemgetter(1))[0]}', 'CNN', 'RNN']\n",
    "acc = [max(scores,key=itemgetter(1))[1], cnn_eval[1], rnn_eval[1]]\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.bar(labels, acc)\n",
    "for i in range(3):\n",
    "    plt.text(labels[i], acc[i] + .02, f'{np.round(acc[i]*100,2)}%', ha='center')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.yticks([i/10 for i in range(5, 11)], [f'{i*10}%' for i in range(5, 11)])\n",
    "plt.title('Top Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect Top Performing Model\n",
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = {'rnn': rnn_model, 'cnn': cnn_model}\n",
    "histories = {'rnn_hist': rnn_history, 'cnn_hist': cnn_history}\n",
    "best = 'c' if cnn_eval[1] > rnn_eval[1] else 'r'\n",
    "best_model = top_models[f'{best}nn']\n",
    "best_model_hist = histories[f'{best}nn_hist']\n",
    "print(f'Top Model Type: {best.upper()}NN')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display history for accuracy\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(best_model_hist.history['acc'])\n",
    "plt.plot(best_model_hist.history['val_acc'])\n",
    "plt.title(f'{best.upper()}NN Model Accuracy')\n",
    "plt.ylabel(f'{best.upper()}NN Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Display history for loss\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(best_model_hist.history['loss'])\n",
    "plt.plot(best_model_hist.history['val_loss'])\n",
    "plt.title(f'{best.upper()}NN Model Loss / Error')\n",
    "plt.ylabel(f'{best.upper()}NN Loss / Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Predictions\n",
    "Create a confusion matrix to inspect the number of accurate and inaccurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(best_model.predict(X_test), axis=1)\n",
    "actual = np.argmax(y_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(actual, preds)\n",
    "cm_plot_labels = [p1.title(), p2.title()]\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, normalize=False, title=f'{best.title()}NN Confusion Matrix')\n",
    "\n",
    "print(f'F1 Score: {f1_score(actual, preds)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 1,780 predictions, the network made 123 errors. A slight majority of the errors were predictions that statements were made by Trump, when they were in fact made by Clinton. However, for this project, the direction of the error is of little consequence. Therefore, the F1-Score is being used as the primary metric.\n",
    "\n",
    "## View List of Inaccurate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all predictions to dataframe\n",
    "preds_df = pd.DataFrame.from_dict({'Predictions': preds, 'Actual': actual})\n",
    "preds_df = preds_df.join(X_test_text.reset_index(drop=True))\n",
    "\n",
    "# Create column to indicate if the prediction matches the actual\n",
    "# 0 = Clinton; 1 = Trump\n",
    "preds_df['Accuracy'] = preds_df['Predictions'] == preds_df['Actual']\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for sorting predictions\n",
    "word_counts = {f'{p1.title()}_Correct':[], \n",
    "               f'{p1.title()}_Incorrect':[], \n",
    "               f'{p2.title()}_Correct':[], \n",
    "               f'{p2.title()}_Incorrect':[]}\n",
    "\n",
    "error_tracking = []\n",
    "\n",
    "# Assign each text sample to dictionary\n",
    "# Print actual speaker and text for each inaccurate prediction\n",
    "for i in preds_df.index:\n",
    "    if preds_df.iloc[i]['Actual'] == 0:\n",
    "        if preds_df.iloc[i]['Accuracy'] == True:\n",
    "            word_counts[f'{p1.title()}_Correct'] += preds_df.iloc[i]['bigrams']\n",
    "        else:\n",
    "            word_counts[f'{p1.title()}_Incorrect'] += preds_df.iloc[i]['bigrams']\n",
    "            error_tracking += preds_df.iloc[i]['bigrams']\n",
    "            print(f'{p1.title()}:')\n",
    "            print(preds_df.iloc[i]['text'], '\\n')\n",
    "    else:\n",
    "        if preds_df.iloc[i]['Accuracy'] == True:\n",
    "            word_counts[f'{p2.title()}_Correct'] += preds_df.iloc[i]['bigrams']\n",
    "        else:\n",
    "            word_counts[f'{p2.title()}_Incorrect'] += preds_df.iloc[i]['bigrams']\n",
    "            error_tracking += preds_df.iloc[i]['bigrams']\n",
    "            print(f'{p2.title()}:')\n",
    "            print(preds_df.iloc[i]['text'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Observations: Do errors highlight similarities?\n",
    "Share your thoughts here.\n",
    "\n",
    "### Visualizing the Differences with Word Clouds\n",
    "The first step is to count the frequency with which each word is used by each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for key in word_index.keys():\n",
    "    word_counts[key] = {p1: 0, p2: 0}\n",
    "    \n",
    "for i in range(preds_df.shape[0]):\n",
    "    for word in preds_df.iloc[i]['bigrams']:\n",
    "        selection = p1 if preds_df.iloc[i]['Actual'] == 0 else p2\n",
    "        try:\n",
    "            word_counts[word.lower()][selection]+=1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "for key in list(word_counts.keys())[:5]:\n",
    "    print(f'{key}: {word_counts[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Word Usage Disparity\n",
    "Rather than simply highlighting the most frequently used words by each candidate, it is more desireable to look at the words with the greatest disparity in ussage between the candidates on a percentage basis. The top 500 words with the greatest percentage disparity will be used.\n",
    "\n",
    "*Note: when a word is used frequently by one candidate but never used by the other, 0.5 is subsituted for 0 when calculating the percentage to avoid null/undefined values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame.from_dict(word_counts).T\n",
    "word_counts_df['difference'] = word_counts_df[p1] - word_counts_df[p2]\n",
    "# word_counts_df['abs_difference'] = np.abs(word_counts_df['difference'])\n",
    "\n",
    "# Replace 0 values with 0.5\n",
    "word_counts_df = word_counts_df.replace(to_replace=0, value=.5)\n",
    "\n",
    "word_counts_df[f'{p1}_portion'] = word_counts_df[p2] / word_counts_df[p1]\n",
    "word_counts_df[f'{p2}_portion'] = word_counts_df[p1] / word_counts_df[p2]\n",
    "word_counts_df['pct_diff'] = word_counts_df[[f'{p2}_portion', f'{p1}_portion']].max(axis=1)\n",
    "\n",
    "# Reset 0.5 values to 0\n",
    "word_counts_df = word_counts_df.replace(to_replace=.5, value=0)\n",
    "\n",
    "word_cloud_df = word_counts_df.sort_values(by='pct_diff', ascending=False)[:1000]\n",
    "word_cloud_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Word Counts to Word List String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from IPython.display import Image as im\n",
    "from PIL import Image\n",
    "\n",
    "# Create list of words based on counts in dataframe\n",
    "p1_cloud = []\n",
    "p2_cloud = []\n",
    "\n",
    "p1_word_cloud_df = word_cloud_df[word_cloud_df['difference'] > 0]\n",
    "p2_word_cloud_df = word_cloud_df[word_cloud_df['difference'] < 0]\n",
    "\n",
    "for i, w in enumerate(list(p1_word_cloud_df.index)):\n",
    "    for j in range(int(list(p1_word_cloud_df[p1])[int(i)])):\n",
    "        p1_cloud.append(w)\n",
    "        \n",
    "for i, w in enumerate(list(p2_word_cloud_df.index)):        \n",
    "    for k in range(int(list(p2_word_cloud_df[p2])[int(i)])):\n",
    "        p2_cloud.append(w)\n",
    "\n",
    "# Words repeated in sequence cause them \n",
    "# to be duplicated in word cloud\n",
    "# Shuffle lists to solve problem\n",
    "np.random.shuffle(p1_cloud)\n",
    "np.random.shuffle(p2_cloud)\n",
    "\n",
    "# Convert lists to strings\n",
    "p1_cloud = ', '.join(p1_cloud)\n",
    "p2_cloud = ', '.join(p2_cloud)\n",
    "\n",
    "# Create cloud for errors\n",
    "error_cloud = []\n",
    "for word in error_tracking:\n",
    "    if len(word)>10: # in word_cloud_df.index[:500]:\n",
    "        error_cloud.append(word)\n",
    "error_cloud = ', '.join(error_cloud)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [\"#FF0000\", \"#FF6347\", \"#DC143C\", \"#0000FF\", \"#0000CD\", \"#4169E1\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "# No shape mask\n",
    "p1_wc = WordCloud(background_color='white', colormap='Blues', max_words=500, width=1000, height=500)\n",
    "p1_wc.generate(p1_cloud)\n",
    "\n",
    "p2_wc = WordCloud(background_color='white', colormap='Reds', max_words=500, width=1000, height=500)\n",
    "p2_wc.generate(p2_cloud)\n",
    "\n",
    "error_wc = WordCloud(background_color='white', colormap=cmap, max_words=500, width=1000, height=500)\n",
    "error_wc.generate(error_cloud)\n",
    "\n",
    "# Display word clouds\n",
    "plt.figure(figsize=(20, 35))\n",
    "plt.imshow(p1_wc)\n",
    "plt.axis('off')\n",
    "plt.title(p1.title())\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 35))\n",
    "plt.imshow(p2_wc)\n",
    "plt.axis('off')\n",
    "plt.title(p2.title())\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 35))\n",
    "plt.imshow(error_wc)\n",
    "plt.axis('off')\n",
    "plt.title('Themes in Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = datetime.datetime.now()\n",
    "print(f'Time to Complete: {T1 - t0}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Document your own observations here.\n",
    "\n",
    "## Conclusions & Next Steps\n",
    "What are your conclusions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
